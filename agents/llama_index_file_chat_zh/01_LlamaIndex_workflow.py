"""
åŸºäºWorkflowçš„LlamaIndexæ–‡ä»¶èŠå¤©Agent - æ•™å­¦æ¼”ç¤º
"""

import os
import base64
from typing import List, Dict, Optional, Any
from dotenv import load_dotenv
from llama_index.core import Document, VectorStoreIndex, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.llms import ChatMessage
from llama_index.core.workflow import (
    Context,
    Event,
    StartEvent,
    StopEvent,
    Workflow,
    step,
)
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from pydantic import BaseModel, Field

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


## å·¥ä½œæµäº‹ä»¶å®šä¹‰

class LogEvent(Event):
    """æ—¥å¿—äº‹ä»¶"""
    msg: str


class InputEvent(StartEvent):
    """è¾“å…¥äº‹ä»¶"""
    msg: str
    file_path: Optional[str] = None
    file_name: Optional[str] = None
    base64_content: Optional[str] = None


class LoadDocumentEvent(Event):
    """æ–‡æ¡£åŠ è½½äº‹ä»¶"""
    file_path: Optional[str] = None
    file_name: Optional[str] = None
    base64_content: Optional[str] = None
    msg: str


class ChatEvent(Event):
    """èŠå¤©äº‹ä»¶"""
    msg: str


class ChatResponseEvent(StopEvent):
    """èŠå¤©å“åº”äº‹ä»¶"""
    response: str
    citations: Dict[int, Dict[str, Any]]
    has_documents: bool


## ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹

class CitationInfo(BaseModel):
    """å¼•ç”¨ä¿¡æ¯"""
    content: str = Field(description="å¼•ç”¨çš„å†…å®¹ç‰‡æ®µ")
    metadata: Dict[str, Any] = Field(description="å¼•ç”¨çš„å…ƒæ•°æ®")


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    response: str = Field(description="å¯¹ç”¨æˆ·çš„å“åº”")
    citations: Dict[int, CitationInfo] = Field(
        default_factory=dict,
        description="å¼•ç”¨ä¿¡æ¯ï¼Œé”®ä¸ºå¼•ç”¨ç¼–å·ï¼Œå€¼ä¸ºå¼•ç”¨è¯¦æƒ…"
    )
    has_documents: bool = Field(description="æ˜¯å¦æœ‰åŠ è½½çš„æ–‡æ¡£")


class VectorFileChatWorkflow(Workflow):
    """åŸºäºå‘é‡ç´¢å¼•çš„æ–‡ä»¶èŠå¤©å·¥ä½œæµ"""
    
    def __init__(
        self,
        timeout: float | None = None,
        verbose: bool = False,
        **workflow_kwargs: Any,
    ):
        super().__init__(timeout=timeout, verbose=verbose, **workflow_kwargs)
        
        # é…ç½®LLM
        self.llm = GoogleGenAI(
            model='gemini-2.0-flash',
            api_key=os.getenv('GOOGLE_API_KEY')
        )
        
        # é…ç½®BGEåµŒå…¥æ¨¡å‹
        self.embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-small-zh-v1.5"
        )
        
        # è®¾ç½®å…¨å±€é…ç½®
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        
        # åˆå§‹åŒ–æ–‡æ¡£å­˜å‚¨
        self.documents = []
        self.index = None
        self.chat_history = []

    @step
    def route(self, ev: InputEvent) -> LoadDocumentEvent | ChatEvent:
        """è·¯ç”±æ­¥éª¤ï¼šåˆ¤æ–­æ˜¯åŠ è½½æ–‡æ¡£è¿˜æ˜¯èŠå¤©"""
        if ev.file_path or ev.base64_content:
            return LoadDocumentEvent(
                file_path=ev.file_path,
                file_name=ev.file_name,
                base64_content=ev.base64_content,
                msg=ev.msg
            )
        return ChatEvent(msg=ev.msg)

    @step
    async def load_document(self, ctx: Context, ev: LoadDocumentEvent) -> ChatEvent:
        """æ–‡æ¡£åŠ è½½æ­¥éª¤"""
        try:
            if ev.file_path:
                ctx.write_event_to_stream(LogEvent(msg=f"æ­£åœ¨åŠ è½½æ–‡æ¡£: {ev.file_path}"))
                
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(ev.file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                file_name = ev.file_name or ev.file_path
                
            elif ev.base64_content:
                ctx.write_event_to_stream(LogEvent(msg=f"æ­£åœ¨è§£æbase64æ–‡æ¡£: {ev.file_name}"))
                
                # è§£ç base64å†…å®¹
                content = base64.b64decode(ev.base64_content).decode('utf-8')
                file_name = ev.file_name
                
            else:
                raise ValueError("å¿…é¡»æä¾›æ–‡ä»¶è·¯å¾„æˆ–base64å†…å®¹")
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            doc = Document(
                text=content,
                metadata={"file_name": file_name}
            )
            
            # æ·»åŠ åˆ°æ–‡æ¡£åˆ—è¡¨
            self.documents.append(doc)
            
            # é‡æ–°æ„å»ºç´¢å¼•
            await self._build_index(ctx)
            
            ctx.write_event_to_stream(LogEvent(msg=f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ: {file_name}"))
            
        except Exception as e:
            ctx.write_event_to_stream(LogEvent(msg=f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}"))
        
        return ChatEvent(msg=ev.msg)

    async def _build_index(self, ctx: Context):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        if not self.documents:
            ctx.write_event_to_stream(LogEvent(msg="âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ç´¢å¼•"))
            return
        
        ctx.write_event_to_stream(LogEvent(msg="æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•..."))
        
        # ä½¿ç”¨å¥å­åˆ†å‰²å™¨
        splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
        
        # åˆ›å»ºå‘é‡ç´¢å¼•
        self.index = VectorStoreIndex.from_documents(
            self.documents,
            transformations=[splitter]
        )
        
        ctx.write_event_to_stream(
            LogEvent(msg=f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£")
        )

    @step
    async def chat(self, ctx: Context, ev: ChatEvent) -> ChatResponseEvent:
        """èŠå¤©æ­¥éª¤"""
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.chat_history.append({"role": "user", "content": ev.msg})
            
            if not self.index:
                response_content = "æŠ±æ­‰ï¼Œè¿˜æ²¡æœ‰åŠ è½½ä»»ä½•æ–‡æ¡£ã€‚è¯·å…ˆåŠ è½½ä¸€ä¸ªæ–‡æ¡£ã€‚"
                self.chat_history.append({"role": "assistant", "content": response_content})
                
                return ChatResponseEvent(
                    response=response_content,
                    citations={},
                    has_documents=False
                )
            
            # åˆ›å»ºæŸ¥è¯¢å¼•æ“
            query_engine = self.index.as_query_engine(
                similarity_top_k=3,
                response_mode="compact"
            )
            
            # æ‰§è¡ŒæŸ¥è¯¢
            ctx.write_event_to_stream(LogEvent(msg="æ­£åœ¨æŸ¥è¯¢æ–‡æ¡£..."))
            response = query_engine.query(ev.msg)
            
            # æå–å¼•ç”¨ä¿¡æ¯
            citations = {}
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for i, node in enumerate(response.source_nodes, 1):
                    citations[i] = {
                        "content": node.text[:200] + "..." if len(node.text) > 200 else node.text,
                        "metadata": node.metadata
                    }
            
            response_content = str(response)
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            self.chat_history.append({"role": "assistant", "content": response_content})
            
            return ChatResponseEvent(
                response=response_content,
                citations=citations,
                has_documents=True
            )
            
        except Exception as e:
            error_msg = f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
            self.chat_history.append({"role": "assistant", "content": error_msg})
            
            return ChatResponseEvent(
                response=error_msg,
                citations={},
                has_documents=bool(self.index)
            )

    def get_chat_history(self) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        return self.chat_history

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.chat_history = []
        print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º")

    def get_document_info(self) -> Dict:
        """è·å–æ–‡æ¡£ä¿¡æ¯"""
        if not self.documents:
            return {"count": 0, "documents": []}
        
        doc_info = []
        for doc in self.documents:
            doc_info.append({
                "file_name": doc.metadata.get("file_name", "æœªçŸ¥"),
                "text_length": len(doc.text),
                "lines": len(doc.text.split('\n'))
            })
        
        return {
            "count": len(self.documents),
            "documents": doc_info
        }


def create_sample_document():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç”¨äºæµ‹è¯•"""
    sample_content = """
# äººå·¥æ™ºèƒ½ç®€ä»‹

äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚

## ä¸»è¦åˆ†æ”¯

### 1. æœºå™¨å­¦ä¹ 
æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚

### 2. æ·±åº¦å­¦ä¹ 
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚

### 3. è‡ªç„¶è¯­è¨€å¤„ç†
è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸“æ³¨äºä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚

## åº”ç”¨é¢†åŸŸ

- è‡ªåŠ¨é©¾é©¶æ±½è½¦
- åŒ»ç–—è¯Šæ–­
- æ¨èç³»ç»Ÿ
- è¯­éŸ³è¯†åˆ«
- å›¾åƒè¯†åˆ«

## æœªæ¥å±•æœ›

éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦è€ƒè™‘ä¼¦ç†å’Œå®‰å…¨é—®é¢˜ã€‚
"""
    
    with open('sample_ai_doc.txt', 'w', encoding='utf-8') as f:
        f.write(sample_content)
    
    return 'sample_ai_doc.txt'


async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºåŸºäºWorkflowçš„æ–‡ä»¶èŠå¤©Agent"""
    print("=== åŸºäºWorkflowçš„LlamaIndexæ–‡ä»¶èŠå¤©Agentæ¼”ç¤º ===\n")
    
    # åˆ›å»ºWorkflow Agent
    agent = VectorFileChatWorkflow()
    ctx = Context(agent)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    sample_file = create_sample_document()
    
    # åŠ è½½æ–‡æ¡£
    handler = agent.run(
        start_event=InputEvent(
            msg="ä½ å¥½ï¼è¯·åŠ è½½è¿™ä¸ªæ–‡æ¡£ã€‚",
            file_path=sample_file,
            file_name="AIä»‹ç»æ–‡æ¡£"
        ),
        ctx=ctx,
    )
    
    # å¤„ç†åŠ è½½æ–‡æ¡£çš„äº‹ä»¶æµ
    async for event in handler:
        if not isinstance(event, StopEvent):
            print(f"ğŸ“ {event.msg}")
    
    # è·å–åŠ è½½ç»“æœ
    load_response = await handler
    
    print(f"\nğŸ“„ æ–‡æ¡£ä¿¡æ¯:")
    doc_info = agent.get_document_info()
    for doc in doc_info["documents"]:
        print(f"  â€¢ {doc['file_name']}: {doc['lines']} è¡Œ, {doc['text_length']} å­—ç¬¦")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ çš„ä¸»è¦åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ",
        "AIçš„æœªæ¥å‘å±•å¦‚ä½•ï¼Ÿ"
    ]
    
    print(f"\nğŸ’¬ å¼€å§‹å¯¹è¯æµ‹è¯•:")
    for i, query in enumerate(test_queries, 1):
        print(f"\n--- æŸ¥è¯¢ {i}: {query} ---")
        
        # åˆ›å»ºèŠå¤©äº‹ä»¶
        chat_handler = agent.run(
            start_event=InputEvent(msg=query),
            ctx=ctx,
        )
        
        # å¤„ç†èŠå¤©äº‹ä»¶æµ
        async for event in chat_handler:
            if not isinstance(event, StopEvent):
                print(f"ğŸ“ {event.msg}")
        
        # è·å–èŠå¤©å“åº”
        result = await chat_handler
        
        print(f"å›å¤: {result.response}")
        
        if result.citations:
            print("å¼•ç”¨:")
            for citation_num, citation_info in result.citations.items():
                print(f"  [{citation_num}] {citation_info['content']}")
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    print(f"\nğŸ“ å¯¹è¯å†å²:")
    history = agent.get_chat_history()
    for msg in history:
        role = "ç”¨æˆ·" if msg["role"] == "user" else "åŠ©æ‰‹"
        print(f"{role}: {msg['content'][:100]}{'...' if len(msg['content']) > 100 else ''}")
    
    print("\n=== æ¼”ç¤ºå®Œæˆ ===")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main()) 